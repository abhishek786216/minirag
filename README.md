# Mini RAG Application

A production-ready Retrieval-Augmented Generation (RAG) application that allows users to upload documents, ask questions, and receive AI-powered answers with citations.

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Frontend      â”‚    â”‚   Backend       â”‚    â”‚  Vector Store   â”‚
â”‚   (HTML/JS)     â”‚â—„â”€â”€â–ºâ”‚   (FastAPI)     â”‚â—„â”€â”€â–ºâ”‚   (Pinecone)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚   LLM Service   â”‚
                       â”‚   (OpenAI)      â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## âœ¨ Features

- **Document Upload**: Support for PDF and TXT files
- **Text Input**: Direct text input for immediate processing
- **Semantic Search**: Vector-based document retrieval
- **Reranking**: Cohere reranker for improved relevance
- **AI Answers**: OpenAI GPT-powered responses with citations
- **Real-time Processing**: Async document processing
- **Production Ready**: Error handling, logging, and monitoring

## ğŸš€ Quick Start

### Prerequisites

- Python 3.8+
- API Keys:
  - OpenAI API Key
  - Pinecone API Key
  - Cohere API Key

### Installation

1. **Clone and setup**:
   ```bash
   git clone <your-repo>
   cd mini-rag
   pip install -r requirements.txt
   ```

2. **Environment Setup**:
   Create `.env` file:
   ```env
   OPENAI_API_KEY=your_openai_key
   PINECONE_API_KEY=your_pinecone_key
   PINECONE_ENVIRONMENT=your_pinecone_env
   PINECONE_INDEX_NAME=minirag-index
   COHERE_API_KEY=your_cohere_key
   ```

3. **Run Application**:
   ```bash
   python main.py
   ```

4. **Access**: Open http://localhost:8000

## ğŸ“Š Configuration

### Vector Database Settings
- **Provider**: Pinecone
- **Index Name**: minirag-index
- **Dimensions**: 1536 (OpenAI text-embedding-ada-002)
- **Metric**: cosine
- **Upsert Strategy**: Batch processing with metadata

### Chunking Strategy
- **Size**: 1000 tokens
- **Overlap**: 200 tokens
- **Method**: Recursive text splitting
- **Metadata**: Source, title, chunk_id, position

### Retrieval Settings
- **Top-k**: 10 initial results
- **Reranker**: Cohere rerank-english-v2.0
- **Final Results**: Top 5 after reranking

### LLM Configuration
- **Provider**: OpenAI
- **Model**: gpt-3.5-turbo
- **Temperature**: 0.1
- **Max Tokens**: 1000

## ğŸ”§ API Endpoints

- `POST /upload`: Upload document files
- `POST /add_text`: Add text directly
- `POST /query`: Query the knowledge base
- `GET /health`: Health check
- `GET /`: Frontend interface

## ğŸ“ Project Structure

```
mini-rag/
â”œâ”€â”€ main.py              # FastAPI application
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ vector_store.py  # Vector database operations
â”‚   â”œâ”€â”€ embeddings.py    # Embedding generation
â”‚   â”œâ”€â”€ retriever.py     # Document retrieval
â”‚   â””â”€â”€ llm.py          # LLM integration
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ chunking.py     # Text chunking logic
â”‚   â””â”€â”€ file_parser.py  # Document parsing
â”œâ”€â”€ static/
â”‚   â”œâ”€â”€ index.html      # Frontend interface
â”‚   â”œâ”€â”€ style.css       # Styling
â”‚   â””â”€â”€ script.js       # JavaScript logic
â”œâ”€â”€ requirements.txt    # Dependencies
â”œâ”€â”€ .env.example       # Environment template
â””â”€â”€ README.md          # Documentation
```

## ğŸš€ Deployment

### Vercel (Recommended)
1. Install Vercel CLI: `npm i -g vercel`
2. Configure `vercel.json`
3. Set environment variables in Vercel dashboard
4. Deploy: `vercel --prod`

### Alternative Platforms
- **Render**: Direct GitHub integration
- **Railway**: Docker or buildpack deployment
- **Heroku**: Procfile-based deployment

## ğŸ” Usage Examples

### Upload Document
```bash
curl -X POST "http://localhost:8000/upload" \
  -F "file=@document.pdf"
```

### Add Text
```bash
curl -X POST "http://localhost:8000/add_text" \
  -H "Content-Type: application/json" \
  -d '{"text": "Your text content here", "title": "Sample Text"}'
```

### Query
```bash
curl -X POST "http://localhost:8000/query" \
  -H "Content-Type: application/json" \
  -d '{"question": "What is the main topic?"}'
```

## ğŸ“ˆ Monitoring & Evaluation

### Success Metrics
- Query response time: < 3 seconds
- Retrieval precision: > 0.8
- User satisfaction: Measured via feedback

### Evaluation Dataset
Includes 5 Q&A pairs for basic functionality testing:

1. **Q**: "What are the key features?"
   **A**: Lists main application capabilities

2. **Q**: "How do I set up the environment?"
   **A**: Environment configuration steps

3. **Q**: "What is the chunking strategy?"
   **A**: Token-based chunking with overlap

4. **Q**: "Which LLM provider is used?"
   **A**: OpenAI with specific model details

5. **Q**: "How is document retrieval performed?"
   **A**: Vector search with reranking process

## âš¡ Performance Considerations

- **Async Processing**: All I/O operations are asynchronous
- **Batch Operations**: Vector upserts in batches of 100
- **Caching**: Embedding cache for repeated queries
- **Rate Limiting**: Built-in API rate limiting
- **Error Handling**: Comprehensive error recovery

## ğŸ”’ Security

- **API Key Management**: Server-side only
- **File Validation**: Type and size restrictions
- **Input Sanitization**: XSS protection
- **CORS**: Configured for production

## ğŸ› ï¸ Development

### Local Development
```bash
# Install dependencies
pip install -r requirements.txt

# Set environment variables
cp .env.example .env
# Edit .env with your API keys

# Run development server
uvicorn main:app --reload --host 0.0.0.0 --port 8000
```

### Testing
```bash
# Test document upload
curl -X POST "http://localhost:8000/upload" -F "file=@test.pdf"

# Test query
curl -X POST "http://localhost:8000/query" \
  -H "Content-Type: application/json" \
  -d '{"question": "test question"}'
```

## ğŸ“ Remarks

### Current Limitations
- **Rate Limits**: Subject to provider API limits
- **File Size**: Max 10MB per file
- **Concurrent Users**: Optimized for moderate load
- **Language**: Primarily English support

### Future Enhancements
- **Multi-language Support**: Add language detection
- **Advanced Chunking**: Semantic-aware chunking
- **User Authentication**: Add user management
- **Conversation Memory**: Maintain chat history
- **Advanced Analytics**: Usage metrics dashboard

### Trade-offs Made
- **Simplicity vs Features**: Focused on core RAG functionality
- **Speed vs Accuracy**: Balanced retrieval speed with quality
- **Cost vs Performance**: Used efficient but cost-effective models

## ğŸ“ Support

For issues and questions:
1. Check the troubleshooting section
2. Review API documentation
3. Open an issue with detailed error logs

## ğŸ“„ License

MIT License - see LICENSE file for details.

---

Built with â¤ï¸ for efficient document Q&A experiences.